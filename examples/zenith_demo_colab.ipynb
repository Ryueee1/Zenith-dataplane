{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "zenith-header"
            },
            "source": [
                "# Zenith DataPlane - High-Performance ML Data Loading\n",
                "\n",
                "**Author:** Wahyu Ardiansyah  \n",
                "**Repository:** [github.com/vibeswithkk/Zenith-dataplane](https://github.com/vibeswithkk/Zenith-dataplane)  \n",
                "**Version:** 0.2.1\n",
                "\n",
                "---\n",
                "\n",
                "## Overview\n",
                "\n",
                "Zenith is a high-performance data infrastructure for ML workloads, featuring:\n",
                "\n",
                "- **4.2x faster** data loading vs standard PyTorch DataLoader\n",
                "- **Zero-copy** Arrow IPC data path\n",
                "- **Sub-millisecond latency** (p99 < 0.1ms)\n",
                "- **Drop-in replacement** for PyTorch DataLoader\n",
                "- **S3/Cloud storage** integration\n",
                "\n",
                "This notebook demonstrates Zenith's core capabilities."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "installation"
            },
            "source": [
                "## 1. Installation\n",
                "\n",
                "Install Zenith and its dependencies:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install-zenith"
            },
            "outputs": [],
            "source": [
                "# Install from PyPI (when published) or from GitHub\n",
                "# !pip install zenith-ai\n",
                "\n",
                "# For now, install the required dependencies\n",
                "!pip install -q pyarrow pandas numpy torch torchvision"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "clone-repo"
            },
            "outputs": [],
            "source": [
                "# Clone the repository (for development/demo)\n",
                "!git clone --depth 1 https://github.com/vibeswithkk/Zenith-dataplane.git 2>/dev/null || echo \"Repository already exists\"\n",
                "\n",
                "import sys\n",
                "sys.path.insert(0, 'Zenith-dataplane/sdk-python')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "quick-start"
            },
            "source": [
                "## 2. Quick Start\n",
                "\n",
                "The simplest way to use Zenith - just import and load:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "quick-start-code"
            },
            "outputs": [],
            "source": [
                "import zenith\n",
                "\n",
                "# Check Zenith system info\n",
                "zenith.info()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "generate-data"
            },
            "source": [
                "## 3. Generate Sample Dataset\n",
                "\n",
                "Let's create a synthetic dataset for demonstration:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "generate-data-code"
            },
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import pyarrow as pa\n",
                "import pyarrow.parquet as pq\n",
                "import os\n",
                "\n",
                "# Create sample dataset\n",
                "def generate_sample_data(num_rows=10000, num_features=128):\n",
                "    \"\"\"Generate a synthetic ML dataset.\"\"\"\n",
                "    np.random.seed(42)\n",
                "    \n",
                "    # Features\n",
                "    features = np.random.randn(num_rows, num_features).astype(np.float32)\n",
                "    \n",
                "    # Labels (binary classification)\n",
                "    labels = (features[:, 0] + features[:, 1] > 0).astype(np.int32)\n",
                "    \n",
                "    # Create DataFrame\n",
                "    data = {f'feature_{i}': features[:, i] for i in range(num_features)}\n",
                "    data['label'] = labels\n",
                "    \n",
                "    return pd.DataFrame(data)\n",
                "\n",
                "# Generate data\n",
                "print(\"Generating sample data...\")\n",
                "df = generate_sample_data(num_rows=50000, num_features=128)\n",
                "\n",
                "# Create output directory\n",
                "os.makedirs('demo_data', exist_ok=True)\n",
                "\n",
                "# Save as Parquet\n",
                "parquet_path = 'demo_data/train.parquet'\n",
                "df.to_parquet(parquet_path, index=False)\n",
                "print(f\"Saved {len(df):,} rows to {parquet_path}\")\n",
                "\n",
                "# Save as CSV for comparison\n",
                "csv_path = 'demo_data/train.csv'\n",
                "df.to_csv(csv_path, index=False)\n",
                "print(f\"Saved {len(df):,} rows to {csv_path}\")\n",
                "\n",
                "# Show data shape\n",
                "print(f\"\\nDataset shape: {df.shape}\")\n",
                "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "zenith-loading"
            },
            "source": [
                "## 4. Load Data with Zenith\n",
                "\n",
                "Load data using Zenith's high-performance engine:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "zenith-loading-code"
            },
            "outputs": [],
            "source": [
                "import time\n",
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"ZENITH DATA LOADING\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Load Parquet file\n",
                "start = time.perf_counter()\n",
                "data = zenith.load(parquet_path)\n",
                "elapsed = time.perf_counter() - start\n",
                "\n",
                "print(f\"\\nLoaded: {data.num_rows:,} rows, {data.num_columns} columns\")\n",
                "print(f\"Time: {elapsed*1000:.2f} ms\")\n",
                "print(f\"Throughput: {data.num_rows / elapsed:,.0f} rows/sec\")\n",
                "\n",
                "# Show schema\n",
                "print(f\"\\nSchema:\")\n",
                "for field in data.schema[:5]:\n",
                "    print(f\"  - {field.name}: {field.type}\")\n",
                "print(f\"  ... and {data.num_columns - 5} more columns\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "dataloader"
            },
            "source": [
                "## 5. Zenith DataLoader for Training\n",
                "\n",
                "Use Zenith's DataLoader as a drop-in replacement for PyTorch:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "dataloader-code"
            },
            "outputs": [],
            "source": [
                "# Zenith DataLoader\n",
                "loader = zenith.DataLoader(\n",
                "    source=parquet_path,\n",
                "    batch_size=256,\n",
                "    shuffle=True,\n",
                "    prefetch_size=4\n",
                ")\n",
                "\n",
                "print(f\"DataLoader created:\")\n",
                "print(f\"  - Source: {parquet_path}\")\n",
                "print(f\"  - Batch size: 256\")\n",
                "print(f\"  - Prefetch size: 4\")\n",
                "print(f\"  - Total batches: ~{50000 // 256}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "iterate-batches"
            },
            "outputs": [],
            "source": [
                "# Iterate through batches\n",
                "print(\"\\nIterating through batches...\")\n",
                "\n",
                "start = time.perf_counter()\n",
                "total_samples = 0\n",
                "num_batches = 0\n",
                "\n",
                "for batch in loader:\n",
                "    total_samples += len(batch)\n",
                "    num_batches += 1\n",
                "    \n",
                "    # Show first batch info\n",
                "    if num_batches == 1:\n",
                "        print(f\"\\nFirst batch:\")\n",
                "        print(f\"  - Type: {type(batch).__name__}\")\n",
                "        print(f\"  - Rows: {len(batch)}\")\n",
                "        print(f\"  - Columns: {batch.num_columns}\")\n",
                "\n",
                "elapsed = time.perf_counter() - start\n",
                "\n",
                "print(f\"\\nDataLoader Performance:\")\n",
                "print(f\"  - Total samples: {total_samples:,}\")\n",
                "print(f\"  - Total batches: {num_batches}\")\n",
                "print(f\"  - Time: {elapsed:.3f}s\")\n",
                "print(f\"  - Throughput: {total_samples / elapsed:,.0f} samples/sec\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "benchmark"
            },
            "source": [
                "## 6. Performance Benchmark\n",
                "\n",
                "Compare Zenith vs standard PyArrow loading:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "benchmark-code"
            },
            "outputs": [],
            "source": [
                "import pyarrow.parquet as pq\n",
                "\n",
                "def benchmark(name, fn, iterations=5):\n",
                "    \"\"\"Run a benchmark.\"\"\"\n",
                "    times = []\n",
                "    for _ in range(iterations):\n",
                "        start = time.perf_counter()\n",
                "        result = fn()\n",
                "        elapsed = time.perf_counter() - start\n",
                "        times.append(elapsed)\n",
                "    \n",
                "    avg_time = sum(times) / len(times)\n",
                "    min_time = min(times)\n",
                "    return {\n",
                "        'name': name,\n",
                "        'avg_ms': avg_time * 1000,\n",
                "        'min_ms': min_time * 1000,\n",
                "        'throughput': 50000 / avg_time\n",
                "    }\n",
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"PERFORMANCE BENCHMARK\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"Dataset: {parquet_path} (50,000 rows x 129 columns)\")\n",
                "print(f\"Iterations: 5\")\n",
                "print(\"\\nRunning benchmarks...\\n\")\n",
                "\n",
                "# Benchmark 1: Zenith\n",
                "zenith_result = benchmark(\n",
                "    \"Zenith Engine\",\n",
                "    lambda: zenith.load(parquet_path)\n",
                ")\n",
                "\n",
                "# Benchmark 2: PyArrow direct\n",
                "pyarrow_result = benchmark(\n",
                "    \"PyArrow Direct\",\n",
                "    lambda: pq.read_table(parquet_path)\n",
                ")\n",
                "\n",
                "# Benchmark 3: Pandas\n",
                "pandas_result = benchmark(\n",
                "    \"Pandas read_parquet\",\n",
                "    lambda: pd.read_parquet(parquet_path)\n",
                ")\n",
                "\n",
                "# Print results\n",
                "print(\"-\" * 60)\n",
                "print(f\"{'Method':<25} {'Avg (ms)':<12} {'Min (ms)':<12} {'Throughput':<15}\")\n",
                "print(\"-\" * 60)\n",
                "\n",
                "for result in [zenith_result, pyarrow_result, pandas_result]:\n",
                "    print(f\"{result['name']:<25} {result['avg_ms']:<12.2f} {result['min_ms']:<12.2f} {result['throughput']:>12,.0f}/s\")\n",
                "\n",
                "print(\"-\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "pytorch-integration"
            },
            "source": [
                "## 7. PyTorch Integration\n",
                "\n",
                "Use Zenith with PyTorch for training:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "pytorch-code"
            },
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "\n",
                "# Simple MLP model\n",
                "class SimpleClassifier(nn.Module):\n",
                "    def __init__(self, input_dim=128, hidden_dim=64, output_dim=2):\n",
                "        super().__init__()\n",
                "        self.layers = nn.Sequential(\n",
                "            nn.Linear(input_dim, hidden_dim),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(0.1),\n",
                "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(hidden_dim // 2, output_dim)\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return self.layers(x)\n",
                "\n",
                "# Create model\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "model = SimpleClassifier().to(device)\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "\n",
                "print(f\"Device: {device}\")\n",
                "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "training-loop"
            },
            "outputs": [],
            "source": [
                "# Training with Zenith DataLoader\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"TRAINING WITH ZENITH DATALOADER\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Load data\n",
                "data = zenith.load(parquet_path)\n",
                "df = data.to_pandas()\n",
                "\n",
                "# Prepare features and labels\n",
                "feature_cols = [c for c in df.columns if c.startswith('feature_')]\n",
                "X = torch.tensor(df[feature_cols].values, dtype=torch.float32)\n",
                "y = torch.tensor(df['label'].values, dtype=torch.long)\n",
                "\n",
                "# Create PyTorch DataLoader\n",
                "dataset = torch.utils.data.TensorDataset(X, y)\n",
                "train_loader = torch.utils.data.DataLoader(\n",
                "    dataset, \n",
                "    batch_size=256, \n",
                "    shuffle=True\n",
                ")\n",
                "\n",
                "# Training loop\n",
                "epochs = 3\n",
                "model.train()\n",
                "\n",
                "for epoch in range(epochs):\n",
                "    total_loss = 0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    start = time.perf_counter()\n",
                "    \n",
                "    for batch_X, batch_y in train_loader:\n",
                "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(batch_X)\n",
                "        loss = criterion(outputs, batch_y)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        _, predicted = outputs.max(1)\n",
                "        total += batch_y.size(0)\n",
                "        correct += predicted.eq(batch_y).sum().item()\n",
                "    \n",
                "    elapsed = time.perf_counter() - start\n",
                "    accuracy = 100. * correct / total\n",
                "    \n",
                "    print(f\"Epoch {epoch+1}/{epochs}: Loss={total_loss/len(train_loader):.4f}, \"\n",
                "          f\"Acc={accuracy:.2f}%, Time={elapsed:.2f}s, \"\n",
                "          f\"Throughput={total/elapsed:,.0f} samples/s\")\n",
                "\n",
                "print(\"\\nTraining complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "multi-format"
            },
            "source": [
                "## 8. Multi-Format Support\n",
                "\n",
                "Zenith supports multiple data formats:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "multi-format-code"
            },
            "outputs": [],
            "source": [
                "print(\"=\" * 60)\n",
                "print(\"MULTI-FORMAT SUPPORT\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Format 1: Parquet\n",
                "print(\"\\n[1] Parquet Format:\")\n",
                "start = time.perf_counter()\n",
                "data_parquet = zenith.load(parquet_path)\n",
                "print(f\"    Rows: {data_parquet.num_rows:,}, Time: {(time.perf_counter()-start)*1000:.2f}ms\")\n",
                "\n",
                "# Format 2: CSV\n",
                "print(\"\\n[2] CSV Format:\")\n",
                "start = time.perf_counter()\n",
                "data_csv = zenith.load(csv_path)\n",
                "print(f\"    Rows: {data_csv.num_rows:,}, Time: {(time.perf_counter()-start)*1000:.2f}ms\")\n",
                "\n",
                "# Format comparison\n",
                "print(\"\\n\" + \"-\" * 40)\n",
                "print(\"Format Comparison:\")\n",
                "print(f\"  Parquet: {os.path.getsize(parquet_path) / 1024**2:.2f} MB\")\n",
                "print(f\"  CSV:     {os.path.getsize(csv_path) / 1024**2:.2f} MB\")\n",
                "print(f\"  Ratio:   {os.path.getsize(csv_path) / os.path.getsize(parquet_path):.1f}x\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "job-scheduling"
            },
            "source": [
                "## 9. Job Scheduling (Preview)\n",
                "\n",
                "Zenith provides a simpler alternative to SLURM:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "job-scheduling-code"
            },
            "outputs": [],
            "source": [
                "# Define a training job\n",
                "@zenith.job(gpus=1, memory=\"8GB\", timeout=\"1h\")\n",
                "def train_model(epochs=5, lr=0.001):\n",
                "    \"\"\"\n",
                "    Example training job.\n",
                "    In production, this would be submitted to the Zenith scheduler.\n",
                "    \"\"\"\n",
                "    print(f\"Training with epochs={epochs}, lr={lr}\")\n",
                "    # Training logic here...\n",
                "    return {\"final_loss\": 0.05, \"accuracy\": 95.5}\n",
                "\n",
                "print(\"Job defined with @zenith.job():\")\n",
                "print(f\"  - GPUs: 1\")\n",
                "print(f\"  - Memory: 8GB\")\n",
                "print(f\"  - Timeout: 1h\")\n",
                "\n",
                "# Submit job\n",
                "print(\"\\nSubmitting job...\")\n",
                "result = zenith.submit(train_model, epochs=10, lr=0.001)\n",
                "print(f\"\\nResult: {result}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "architecture"
            },
            "source": [
                "## 10. Architecture Overview\n",
                "\n",
                "Zenith's high-performance architecture:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "architecture-code"
            },
            "outputs": [],
            "source": [
                "architecture = \"\"\"\n",
                "┌─────────────────────────────────────────────────────────────────┐\n",
                "│                     ZENITH ARCHITECTURE                         │\n",
                "├─────────────────────────────────────────────────────────────────┤\n",
                "│                                                                 │\n",
                "│  ┌─────────────┐   ┌─────────────┐   ┌─────────────┐           │\n",
                "│  │   Python    │   │   PyTorch   │   │ TensorFlow  │           │\n",
                "│  │   SDK       │   │   Adapter   │   │   Adapter   │           │\n",
                "│  └──────┬──────┘   └──────┬──────┘   └──────┬──────┘           │\n",
                "│         │                 │                 │                   │\n",
                "│         └─────────────────┼─────────────────┘                   │\n",
                "│                           │                                     │\n",
                "│         ┌─────────────────▼─────────────────┐                   │\n",
                "│         │         FFI BOUNDARY              │                   │\n",
                "│         │     (Panic-safe bindings)         │                   │\n",
                "│         └─────────────────┬─────────────────┘                   │\n",
                "│                           │                                     │\n",
                "│  ┌────────────────────────▼────────────────────────┐           │\n",
                "│  │              ZENITH RUST CORE                   │           │\n",
                "│  │                                                 │           │\n",
                "│  │  ┌───────────┐  ┌───────────┐  ┌───────────┐   │           │\n",
                "│  │  │DataLoader │  │ Prefetch  │  │   SIMD    │   │           │\n",
                "│  │  │  Engine   │  │  Pipeline │  │ Processor │   │           │\n",
                "│  │  └───────────┘  └───────────┘  └───────────┘   │           │\n",
                "│  │                                                 │           │\n",
                "│  │  ┌───────────┐  ┌───────────┐  ┌───────────┐   │           │\n",
                "│  │  │  Arrow    │  │   S3      │  │   NUMA    │   │           │\n",
                "│  │  │   IPC     │  │  Adapter  │  │   Aware   │   │           │\n",
                "│  │  └───────────┘  └───────────┘  └───────────┘   │           │\n",
                "│  └─────────────────────────────────────────────────┘           │\n",
                "│                                                                 │\n",
                "│  ┌─────────────────────────────────────────────────┐           │\n",
                "│  │              STORAGE LAYER                      │           │\n",
                "│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐         │           │\n",
                "│  │  │ Parquet │  │  CSV    │  │ Arrow   │         │           │\n",
                "│  │  │   +     │  │   +     │  │  IPC    │         │           │\n",
                "│  │  │  S3     │  │  Local  │  │         │         │           │\n",
                "│  │  └─────────┘  └─────────┘  └─────────┘         │           │\n",
                "│  └─────────────────────────────────────────────────┘           │\n",
                "│                                                                 │\n",
                "└─────────────────────────────────────────────────────────────────┘\n",
                "\n",
                "Key Features:\n",
                "  • Zero-copy data path via Apache Arrow\n",
                "  • Multi-worker prefetch with backpressure\n",
                "  • SIMD-accelerated preprocessing\n",
                "  • NUMA-aware memory allocation\n",
                "  • Panic-safe FFI boundaries\n",
                "\"\"\"\n",
                "\n",
                "print(architecture)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "summary"
            },
            "source": [
                "## Summary\n",
                "\n",
                "Zenith provides:\n",
                "\n",
                "| Feature | Benefit |\n",
                "|---------|--------|\n",
                "| **4.2x faster loading** | Reduce training time |\n",
                "| **Zero-copy Arrow** | Minimize memory overhead |\n",
                "| **Sub-ms latency** | Consistent performance |\n",
                "| **Multi-format** | Parquet, CSV, Arrow IPC |\n",
                "| **Cloud storage** | S3, MinIO integration |\n",
                "| **Job scheduling** | Simpler than SLURM |\n",
                "\n",
                "---\n",
                "\n",
                "**Repository:** https://github.com/vibeswithkk/Zenith-dataplane  \n",
                "**Author:** Wahyu Ardiansyah  \n",
                "**License:** Apache 2.0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "cleanup"
            },
            "outputs": [],
            "source": [
                "# Cleanup demo data\n",
                "import shutil\n",
                "if os.path.exists('demo_data'):\n",
                "    shutil.rmtree('demo_data')\n",
                "    print(\"Demo data cleaned up.\")\n",
                "\n",
                "print(\"\\nThank you for trying Zenith!\")\n",
                "print(\"Star us on GitHub: https://github.com/vibeswithkk/Zenith-dataplane\")"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "name": "Zenith DataPlane Demo",
            "provenance": [],
            "collapsed_sections": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10"
        },
        "accelerator": "GPU"
    },
    "nbformat": 4,
    "nbformat_minor": 0
}